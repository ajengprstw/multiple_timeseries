---
title: ""
author: "Ajeng Prastiwi, Allissa Rahman"
date: "`r format(Sys.Date(), '%B %e, %Y')`"
output:
  html_document:
    df_print: paged
    highlight: tango
    theme: cosmo
    toc: yes
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
# clear-up the environment
rm(list = ls())
# chunk options
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  comment = "#>"
)
options(scipen = 99)
```

# Introduction
There are many things we can do with data. One of them is to forecast data based on information from the previous data. 
An industry that commonly uses forecasting methods is the economic industry, for instance to forecast monthly inflation, sales, losses, etc. 
To do this forecasting, it usually uses time sequential data so that the time variable is a very crucial variable because the order pattern is very considered.
To process this sequential data  so that we can get the forecast results, an appropriate data analysis method is needed, namely Time Series Analysis.

Most of the cases encountered were using univariate time series which only consists of 2 variables and 1 object. 
Unfortunately, real world use cases not always as simple as that. 
When there are multiple variables and objects at play, it would be inefficient to perform univariate time series, so we have to handle all of them at the same time using Multiple Time Series.

In this case, we are provided a walmart store sales dataset from  Kaggle (https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data). 
With this dataset, we are going to help them in solving store sales forecasting problems in order to improve their business strategies. 
We will solve this problem using direct forecasting method combined with machine learning method.

## Multiple Time Series
Multiple Time Series has more than one time-dependent variable. 
Each variable depends not only on its past values but also has some dependency on other variables. 
This dependency is used for forecasting future values[1].

Multiple time series data of the kind described may be available in different locations. 
For example, we may have parallel daily mortality, temperature, and black smoke data over different metropolitan areas in England and other European countries. 
We would certainly expect that the structure of the relationships among the three time series would be similar in the different locations. 
Thus, it would be interesting to model these parallel multiple time series together in a hierarchic framework so that information from one location can be utilized to help model the relationships and forecast future observations in other locations[5].

The benefits to modeling multiple time series in one go with a single model or ensemble of models include (a) modeling simplicity, (b) potentially more robust results from pooling data across time series, and (c) solving the cold-start problem when few data points are available for a given time series[6].

One of the method to conduct this multiple time series forecasting is to use machine learning model.
By using the machine learning method, we should transform the data so that it can be processed by supervised learning using various regression algorithm.
There are four strategies for multiple time series forescasting :

1. Recursive strategy
Recursive strategy is the oldest and most intuitive strategy to forecast a time series multiple steps ahead[10].
This strategy trains first a one step model then uses it recursively for returning a multistep prediction.
The deficiency of this strategy is its sensitivity to the estimation error, since estimated values, instead of actual ones, are more and more used when we get further in the future.

2. Direct strategy
Direct strategy consists of forecasting each horizon independently from the others. 
This strategy does not use any approximated values to compute the forecasts, being then immune to the accumulation of errors. 
However, the models are learned independently inducing a conditional independence of the H forecasts. 
This affects the forecasting accuracy as it prevents the strategy from considering complex dependencies between the variables
Unfortunately, this strategy demands a large computational time since there are as many models to learn as the size of the horizon.


3. DirRec strategy
The DirRec strategy combines the architectures and the principles underlying the Direct and the Recursive strategies. DirRec computes the forecasts with different models for every horizon (like the Direct strategy) and, at each time step, it enlarges the set of inputs by adding variables corresponding to the forecasts of the previous step (like the Recursive strategy). However, note that unlike the two previous strategies, the embedding size n is not the same for all the horizons.

4. Multiple output strategies
  a. MIMO Strategy
      The Multi-Input Multi-Output (MIMO) strategy (also known as Joint strategy [9]) avoids the simplistic assumption of conditional independence between future values made by the Direct strategy by learning a single multiple-output model.The rationale of the MIMO strategy is to model, between the predicted values, the stochastic dependency characterizing the time series. This strategy avoids the conditional independence assumption made by the direct strategy as well as the accumulation of errors which plagues the recursive strategy.
   b. DIRMO Strategy
      The DIRMO strategy [10,9] aims to preserve the most appealing aspects of DIRect and MIMO strategies by partitioning the horizon H in several blocks, and using MIMO to forecast the values inside each Machine Learning Strategies for Time Series Forecasting block.

Overall, All these strategies learn the dependency between the past and the future in different manners by making specific assumptions. 

## The purpose of `forecastML`
In this project, we are using package `forecastML` to perform Time Series Forecasting with Machine Learning methods. 
The purpose of `forecastML` is to simplify the process of multi-step-ahead forecasting with standard machine learning algorithms. `forecastML` supports lagged, dynamic, static, and grouping features for modeling single and grouped numeric or factor/sequence time series. In addition, simple wrapper functions are used to support model-building with most R packages. This approach to forecasting is inspired by Bergmeir, Hyndman, and Koo's (2018) paper entitled "A note on the validity of cross-validation for evaluating autoregressive time series prediction"[2]

## Learning Objectives

The goal of this article is to help you:
-to understand the main idea of package `forecastML`
-to understand the steps of performing multiple time series forecasting using `xgboost` and `forecastML`  

## Library

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(lubridate)
library(padr)
library(xgboost)
library(forecastML)
```

# Data Preprocess using `forecastML`

## Lagged Features

## Nested cross validation

# Model Training

## XGBoost

# Model Performance

# Conslusion

# Reference
- [1] https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/ 
- [2] https://cran.r-project.org/web/packages/forecastML/index.html
- [3] Sseguya, Raymond. Forecasting anomalies in time series data from online production environments. In: Department of Computer and Information Science
Master’s thesis, 30 ECTS (2020). http://www.diva-portal.org/smash/get/diva2:1436042/FULLTEXT01.pdf
- [4] Whittle, P. “The Analysis of Multiple Stationary Time Series.” Journal of the Royal Statistical Society. Series B (Methodological), vol. 15, no. 1, 1953, pp. 125–139. JSTOR, www.jstor.org/stable/2983728. Accessed 25 Nov. 2020.
- [5] Tiao, G. C. “An Introduction to Multiple Time Series Analysis.” Medical Care, vol. 31, no. 5, 1993, pp. YS71–YS74. JSTOR, www.jstor.org/stable/3766151. Accessed 25 Nov. 2020.
- [6] https://cran.r-project.org/web/packages/forecastML/vignettes/grouped_forecast.html
- [7] A. Galicia, R. Talavera-Llames, A. Troncoso, I. Koprinska, F. Martínez-Álvarez, Multi-step forecasting for big data time series based on ensemble learning, Knowledge-Based Systems, Volume 163,2019,Pages 830-841,ISSN 0950-7051, https://doi.org/10.1016/j.knosys.2018.10.009.
- [8] Bontempi, Gianluca & Ben Taieb, Souhaib & Le Borgne, Yann-Aël. (2013). Machine Learning Strategies for Time Series Forecasting. 10.1007/978-3-642-36318-4_3. 
- [9] Kline, D.M.: Methods for multi-step time series forecasting with neural networks.
In: Peter Zhang, G. (ed.) Neural Networks in Business Forecasting, pp. 226–250.
Information Science Publishing (2004)
- [10] Weigend and Gershenfeld, 1994; Sorjamaa et al., 2007; Cheng et al., 2006; Tiao and Tsay, 1994; Kline, 2004; Hamzaebi et al., 2009

```{r}
walmart <- read_csv("data/train_walmart.csv")
head(walmart)
```

The daya contains 421550 observation and 5 variables. Here some description of each feature:

- Store: the store number
- Depth: the department number
- Date: the week 
- Weekly_Sales: sales for the given department in the given store
- IsHoliday: whether the week is a special holiday week

```{r}
data_clean <- walmart %>% 
                 mutate(Date = ymd(Date)) %>% 
                 filter(Store %in% 1:5) %>%
                 group_by(Store,Date) %>% 
                 summarise(Sales = mean(Weekly_Sales)) %>% 
                 ungroup()
head(data_clean)
```

Now we can visualize data by Store from the train dataset.
```{r}
# visualize train
ggplot(data_clean, aes(x = Date, y = Sales)) +
  geom_line() +
  facet_wrap(~ Store, ncol = 2, scales = "free")+
  labs(title = "Weekly Sales by Store (Train Dataset)", x = NULL, y = NULL, colour = NULL)+
  tidyquant::theme_tq()
```

Package forecastML have `fill_gaps()` to fill rows for the missing dates.
```{r}
data_walmart <- forecastML::fill_gaps(data_clean, 
                                      date_col = 2,
                                      frequency = "1 week", 
                                      groups = "Store")
```

Let's see if we have missing dates:
```{r}
nrow(data_clean)
nrow(data_walmart)
```

split data train data test
```{r}
train <- data_walmart %>% 
              group_by(Store) %>% 
              filter(row_number() <= n() - 4*6) %>% 
              ungroup()
test <- data_walmart %>% 
             group_by(Store) %>% 
             filter(row_number() > n() - 4*6) %>% 
             ungroup()
```


The number observation of `data_clean` and `data walmart the have same number. So we can say that we don't have missing dates. Next, we will create model training with lagged

```{r}
data_train <- forecastML::create_lagged_df(data_walmart, 
                                           type = "train", 
                                           outcome_col = 3,
                                           horizons = c(1,4,12),
                                           lookback = 1:48,
                                           dates = data_walmart$Date, 
                                           frequency = "1 week",
                                           dynamic_features = c("week","month"),
                                           groups = "Store"
                                           )
head(data_train)
```

```{r}
data_test <- forecastML::create_lagged_df(test, 
                                           type = "train", 
                                           outcome_col = 3,
                                           horizons = c(1,4,12),
                                           lookback = 1:48,
                                           dates = test$Date, 
                                           frequency = "1 week",
                                           dynamic_features = c("week","month"),
                                           groups = "Store"
                                           )
head(data_test)
```


```{r}
plot(data_train)
```

```{r}
windows <- forecastML::create_windows(data_train, 
                                      window_length = 4*6, 
                                      skip = 4,
                                      include_partial_window = F)

plot(windows, data_train) + theme(legend.position = "none")

```


```{r}
plot(windows, 
     data_train,
     group_filter = "Store == 1"
     ) + 
  theme(legend.position = "none")
```

```{r}
model_function <- function(data, outcome_col = 1) {
  
data <- data[!is.na(data[, 1]), ]

data_train <- data %>% 
              head(-4*6)
data_test <- data %>% 
             tail(4*6)

# convert data to matrix
train_matrix <- data.matrix(data_train[,-1, drop = F])
test_matrix <- data.matrix(data_test[,-1, drop = F])

# conver data to Dmatrix
dtrain <- xgb.DMatrix(data = train_matrix, 
                      label = as.matrix(data_train[,1, drop =F]))
dtest <- xgb.DMatrix(data = test_matrix, 
                     label = as.matrix(data_test[,1, drop =F]))

params <- list("objective" = "reg:squarederror")
watchlist <- list(train = dtrain, test = dtest)
  
set.seed(100)
model <- xgboost::xgb.train(data = dtrain, 
                            params = params, 
                            max.depth = 8, 
                            nthread = 2, 
                            nrounds = 30,
                            metrics = "rmse",
                            early_stopping_rounds = 5, 
                            watchlist = watchlist,
                            verbosity = 0)

  return(model)
}
```


```{r}
model_results <- forecastML::train_model(lagged_df = data_train,
                                            windows = windows,
                                            model_name = "xgboost",
                                            model_function = model_function, 
                                            use_future = FALSE)
```

```{r}
summary(model_results$horizon_1$window_1$model)
```

```{r}
prediction_function <- function(model, data_features) {
  x <- xgb.DMatrix(data = data.matrix(data_features))
  data_pred <- data.frame("y_pred" = predict(model, x))  
  return(data_pred)
}
```


```{r}
data_pred <- predict(model_results, prediction_function = list(prediction_function), data = data_test) 
```

```{r}
dat_new <- data_pred %>% 
  select(date_indices, Store, Sales, Sales_pred, model_forecast_horizon) %>% 
  pivot_longer(cols = c(Sales, Sales_pred), names_to = "result")
```


```{r}
ggplot(dat_new %>% filter(model_forecast_horizon %in% c(1:3)), aes(x = date_indices, y = value))+
  geom_line( aes(group = result, col = result))+
  facet_wrap(~Store+model_forecast_horizon, scales = "free")
```



```{r}
#plot(data_pred)
```

```{r}
#plot(data_pred, facet = group ~ horizon, windows = 2)
```


```{r}
data_error <- forecastML::return_error(data_pred)

plot(data_error, type = "window", metric = "mape") %>% 
  plotly::ggplotly()
```

```{r}
plot(data_forecasts)
```

```{r}
windows <- forecastML::create_windows(data_train, window_length = 0)

plot(windows, data_train) + theme(legend.position = "none")
```


```{r}
model_results_test <- forecastML::train_model(lagged_df = data_test, 
                                               windows = windows,
                                               model_name = "xgboost",
                                               model_function = model_function,
                                               use_future = FALSE)
```

```{r}
data_pred_test <- predict(model_results_test, prediction_function = list(prediction_function), data = data_test)
```


```{r}
pred_test <- data_pred_test %>% 
  select(date_indices, Store, Sales, Sales_pred, model_forecast_horizon) %>% 
  pivot_longer(cols = c(Sales, Sales_pred), names_to = "result")
```


```{r}
ggplot(pred_test %>% filter(model_forecast_horizon %in% c(1:3)), aes(x = date_indices, y = value))+
  geom_line( aes(group = result, col = result))+
  facet_wrap(~Store+model_forecast_horizon, scales = "free")
```




## Random Forest


```{r}
data_train <- forecastML::create_lagged_df(data_walmart, 
                                           type = "train", 
                                           outcome_col = 3,
                                           horizons = c(1,4,12),
                                           lookback = 1:48,
                                           dates = data_walmart$Date, 
                                           frequency = "1 week",
                                           dynamic_features = c("week","month"),
                                           #groups = "Store"
                                           )
head(data_train)
```


```{r}
model_rf <- function(data, outcome_cols = 1) {
   outcome_names <- names(data)[outcome_cols]
  model_formula <- formula(paste0(outcome_names,  "~ ."))

  model <- randomForest::randomForest(formula = model_formula,
                                      data = data, 
                                      ntree = 200)
  return(model) 
}
```


```{r}
model_results_rf <- forecastML::train_model(lagged_df = data_train, windows, 
                                           model_rf, model_name = "RF",
                                           use_future = F)
```

```{r}
prediction_rf <- function(model, data_features) {

  data_pred <- data.frame("y_pred" = predict(model, data_features))
  return(data_pred)
}
data_results <- predict(model_results_rf, 
                        prediction_function = list(prediction_rf), 
                        data = data_train)
```

```{r}
data_results$Sales_pred <- round(data_results$Sales_pred,0)
head(data_results)
```

```{r}
MLmetrics::MAPE(y_pred = data_results$Sales_pred,y_true = data_results$Sales)
```
```{r}
ggplot(data = data_results,mapping = aes(x = date_indices, y = Sales)) +
  geom_line(col = "red")+
  geom_line(data = data_results,mapping = aes(x = date_indices, y = Sales_pred), col = "blue")
```

```{r}
plot(data_results, type = "prediction", horizons = c(1,4,12))
```




=============================================

# Reference

```{r}
library(forecastML)
data("data_buoy_gaps", package = "forecastML")

head(data_buoy_gaps)

range(data_buoy_gaps$date)
```

```{r}
data <- forecastML::fill_gaps(data_buoy_gaps, date_col = 1, frequency = '1 day', 
                              groups = 'buoy_id', static_features = c('lat', 'lon'))
data$day <- lubridate::mday(data$date)
data$year <- lubridate::year(data$date)
```

```{r}
data$buoy_id <- as.numeric(factor(data$buoy_id))
outcome_col <- 1  # The column position of our 'wind_spd' outcome (after removing the 'date' column).

horizons <- c(1, 7, 30)  # Forecast 1, 1:7, and 1:30 days into the future.

lookback <- c(1:30, 360:370)  # Features from 1 to 30 days in the past and annually.

dates <- data$date  # Grouped time series forecasting requires dates.
data$date <- NULL  # Dates, however, don't need to be in the input data.

frequency <- "1 day"  # A string that works in base::seq(..., by = "frequency").

dynamic_features <- c("day", "year")  # Features that change through time but which will not be lagged.

groups <- "buoy_id"  # 1 forecast for each group or buoy.

static_features <- c("lat", "lon")
```

```{r}
type <- "train"  # Create a model-training dataset.

data_train <- forecastML::create_lagged_df(data, 
                                           type = type,
                                           outcome_col = outcome_col,
                                           horizons = horizons, 
                                           lookback = lookback,
                                           dates = dates, 
                                           frequency = frequency,
                                           dynamic_features = dynamic_features,
                                           groups = groups, 
                                           static_features = static_features, 
                                           use_future = FALSE)

head(data_train)
head(data_train$horizon_1)
```



```{r}
windows <- forecastML::create_windows(data_train, window_length = 365, skip = 730,
                                      include_partial_window = FALSE)

p <- plot(windows, data_train) + theme(legend.position = "none")
p
```
```{r}
model_function <- function(data, outcome_col = 1) {
  
  # xgboost cannot handle missing outcomes data.
  data <- data[!is.na(data[, outcome_col]), ]

  indices <- 1:nrow(data)
  
  set.seed(224)
  train_indices <- sample(1:nrow(data), ceiling(nrow(data) * .8), replace = FALSE)
  test_indices <- indices[!(indices %in% train_indices)]

  data_train <- xgboost::xgb.DMatrix(data = as.matrix(data[train_indices, 
                                                           -(outcome_col), drop = FALSE]),
                                     label = as.matrix(data[train_indices, 
                                                            outcome_col, drop = FALSE]))

  data_test <- xgboost::xgb.DMatrix(data = as.matrix(data[test_indices, 
                                                          -(outcome_col), drop = FALSE]),
                                    label = as.matrix(data[test_indices, 
                                                           outcome_col, drop = FALSE]))

  params <- list("objective" = "reg:squarederror")
  watchlist <- list(train = data_train, test = data_test)
  
  set.seed(224)
  model <- xgboost::xgb.train(data = data_train, params = params, 
                              max.depth = 8, nthread = 2, nrounds = 30,
                              metrics = "rmse", verbose = 0, 
                              early_stopping_rounds = 5, 
                              watchlist = watchlist)

  return(model)
}
```

```{r}
model_results_cv <- forecastML::train_model(lagged_df = data_train,
                                            windows = windows,
                                            model_name = "xgboost",
                                            model_function = model_function, 
                                            use_future = FALSE)
```

```{r}
model_results_cv
```


```{r}
summary(model_results_cv$horizon_1$window_1$model)
```


```{r}
prediction_function <- function(model, data_features) {
  x <- xgboost::xgb.DMatrix(data = as.matrix(data_features))
  data_pred <- data.frame("y_pred" = predict(model, x),
                          "y_pred_lower" = predict(model, x) - 2,  # Optional; in practice, forecast bounds are not hard coded.
                          "y_pred_upper" = predict(model, x) + 2)  # Optional; in practice, forecast bounds are not hard coded.
  return(data_pred)
}
```


```{r}
data_pred_cv <- predict(model_results_cv, prediction_function = list(prediction_function), data = data_train)
```


```{r}
plot(data_pred_cv)
```


