---
title: ""
author: "Ajeng Prastiwi, Allissa Rahman"
date: "`r format(Sys.Date(), '%B %e, %Y')`"
output:
  html_document:
    df_print: paged
    highlight: tango
    theme: cosmo
    toc: yes
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
# clear-up the environment
rm(list = ls())
# chunk options
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  comment = "#>"
)
options(scipen = 99)
```


# Introduction
There are many things we can do with data. One of them is to forecast data based on information from the previous data. 
An industry that commonly uses forecasting methods is the economic industry, for instance to forecast monthly inflation, sales, losses, etc. 
To do this forecasting, it usually uses time sequential data so that the time variable is a very crucial variable because the order pattern is very considered.
To process this sequential data  so that we can get the forecast results, an appropriate data analysis method is needed, namely Time Series Analysis.

Most of the cases encountered were using univariate time series which only consists of 2 variables and 1 object. 
Unfortunately, real world use cases not always as simple as that. 
When there are multiple variables and objects at play, it would be inefficient to perform univariate time series, so we have to handle all of them at the same time using Multiple Time Series.

In this case, we are provided a Hotel booking dataset from  Kaggle (https://www.kaggle.com/jessemostipak/hotel-booking-demand). 
With this dataset, we are going to help them in solving store sales forecasting problems in order to improve their business strategies. 
We will solve this problem using direct forecasting method combined with machine learning method.

## Multiple Time Series
Multiple Time Series has more than one time-dependent variable. 
Each variable depends not only on its past values but also has some dependency on other variables. 
This dependency is used for forecasting future values[1].

Multiple time series data of the kind described may be available in different locations. 
For example, we may have parallel daily mortality, temperature, and black smoke data over different metropolitan areas in England and other European countries. 
We would certainly expect that the structure of the relationships among the three time series would be similar in the different locations. 
Thus, it would be interesting to model these parallel multiple time series together in a hierarchic framework so that information from one location can be utilized to help model the relationships and forecast future observations in other locations[5].

The benefits to modeling multiple time series in one go with a single model or ensemble of models include (a) modeling simplicity, (b) potentially more robust results from pooling data across time series, and (c) solving the cold-start problem when few data points are available for a given time series[6].

One of the method to conduct this multiple time series forecasting is to use machine learning model.
By using the machine learning method, we should transform the data so that it can be processed by supervised learning using various regression algorithm.
There are four strategies for multiple time series forescasting :

1. Recursive strategy
Recursive strategy is the oldest and most intuitive strategy to forecast a time series multiple steps ahead[10].
This strategy trains first a one step model then uses it recursively for returning a multistep prediction.
The deficiency of this strategy is its sensitivity to the estimation error, since estimated values, instead of actual ones, are more and more used when we get further in the future.

2. Direct strategy
Direct strategy consists of forecasting each horizon independently from the others. 
This strategy does not use any approximated values to compute the forecasts, being then immune to the accumulation of errors. 
However, the models are learned independently inducing a conditional independence of the H forecasts. 
This affects the forecasting accuracy as it prevents the strategy from considering complex dependencies between the variables
Unfortunately, this strategy demands a large computational time since there are as many models to learn as the size of the horizon.


3. DirRec strategy
The DirRec strategy combines the architectures and the principles underlying the Direct and the Recursive strategies. DirRec computes the forecasts with different models for every horizon (like the Direct strategy) and, at each time step, it enlarges the set of inputs by adding variables corresponding to the forecasts of the previous step (like the Recursive strategy). However, note that unlike the two previous strategies, the embedding size n is not the same for all the horizons.

4. Multiple output strategies
  a. MIMO Strategy
      The Multi-Input Multi-Output (MIMO) strategy (also known as Joint strategy [9]) avoids the simplistic assumption of conditional independence between future values made by the Direct strategy by learning a single multiple-output model.The rationale of the MIMO strategy is to model, between the predicted values, the stochastic dependency characterizing the time series. This strategy avoids the conditional independence assumption made by the direct strategy as well as the accumulation of errors which plagues the recursive strategy.
   b. DIRMO Strategy
      The DIRMO strategy [10,9] aims to preserve the most appealing aspects of DIRect and MIMO strategies by partitioning the horizon H in several blocks, and using MIMO to forecast the values inside each Machine Learning Strategies for Time Series Forecasting block.

Overall, All these strategies learn the dependency between the past and the future in different manners by making specific assumptions. 

## The purpose of `forecastML`
In this project, we are using package `forecastML` to perform Time Series Forecasting with Machine Learning methods. 
The purpose of `forecastML` is to simplify the process of multi-step-ahead forecasting with standard machine learning algorithms. `forecastML` supports lagged, dynamic, static, and grouping features for modeling single and grouped numeric or factor/sequence time series. In addition, simple wrapper functions are used to support model-building with most R packages. This approach to forecasting is inspired by Bergmeir, Hyndman, and Koo's (2018) paper entitled "A note on the validity of cross-validation for evaluating autoregressive time series prediction"[2]

## Learning Objectives

The goal of this article is to help you:
-to understand the main idea of package `forecastML`
-to understand the steps of performing multiple time series forecasting using `xgboost` and `forecastML`  

## Library

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(lubridate)
library(padr)
library(xgboost)
library(forecastML)
```

# Data Preprocess using `forecastML`

## Lagged Features
Lag features are the classical way that time series forecasting problems are transformed into supervised learning problems.
The simplest approach is to predict the value at the next time (t+1) given the value at the previous time (t-1)[3].

## Nested cross validation

# Model Training

## XGBoost

## ....

# Model Performance

# Conslusion

# Reference
- [1] https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/ 
- [2] https://cran.r-project.org/web/packages/forecastML/index.html
- Sseguya, Raymond. Forecasting anomalies in time series data from online production environments. In: Department of Computer and Information Science
Masterâ€™s thesis, 30 ECTS (2020). http://www.diva-portal.org/smash/get/diva2:1436042/FULLTEXT01.pdf
- [3] https://machinelearningmastery.com/basic-feature-engineering-time-series-data-python/ 

## Hotel

```{r}
hotel <- read_csv("data/hotel_bookings.csv") %>% 
         mutate_if(is.character, as.factor) %>% 
         unite("arrival_date", arrival_date_year, 
               arrival_date_month, 
               arrival_date_day_of_month, sep = "-") %>% 
         mutate(arrival_date = ymd(arrival_date)) %>% 
         filter(market_segment %>% 
                  str_detect("TA|Direct")) %>% 
         group_by(arrival_date, hotel) %>% 
         summarise(demand = n()) %>% 
         ungroup()
head(hotel)

```

```{r}
hotel_clean <- hotel %>% 
  filter(demand < 200)
```

```{r}
hotel_clean2 <- fill_gaps(hotel_clean, 
                          date_col = 1, 
                          frequency = "1 day",
                           groups = "hotel")
```
Let's see if we have missing dates:
```{r}
nrow(hotel_clean)
nrow(hotel_clean2)

colSums(is.na(hotel_clean2))
```

```{r}
library(tidyr)
hotel_clean2 <- hotel_clean2 %>% 
                fill(demand, .direction = "updown")
colSums(is.na(hotel_clean2))
```

Now we can visualize data by Store from the train dataset.
```{r}
# visualize train
ggplot(hotel_clean2, aes(x = arrival_date, y = demand)) +
  geom_line() +
  facet_wrap(~ hotel, ncol = 2, scales = "free")+
  labs(title = "Weekly Sales by Store (Train Dataset)", x = NULL, y = NULL, colour = NULL)+
  tidyquant::theme_tq()
```

Package forecastML have `fill_gaps()` to fill rows for the missing dates.
```{r}
data_hotel <- forecastML::fill_gaps(hotel_clean2, 
                                      date_col = 1,
                                      frequency = "1 day", 
                                      groups = "hotel")
```



split data train data test
```{r}
train <- data_hotel %>% 
              group_by(hotel) %>% 
              filter(row_number() <= n() - 30) %>% 
              ungroup()
test <- data_hotel %>% 
             group_by(hotel) %>% 
             filter(row_number() > n() - 30) %>% 
             ungroup()
```


The number observation of `data_clean` and `data walmart the have same number. So we can say that we don't have missing dates. Next, we will create model training with lagged

```{r}
data_train <- forecastML::create_lagged_df(data_hotel, 
                                           type = "train", 
                                           outcome_col = 3,
                                           horizons = c(1,7),
                                           lookback = 1:7,
                                           dates = data_hotel$arrival_date, 
                                           frequency = "1 day",
                                           dynamic_features = c("day","week"),
                                           groups = "hotel"
                                           )
head(data_train)
```


```{r}
data_test <- forecastML::create_lagged_df(train, 
                                          type = "forecast", 
                                           outcome_col = 3,
                                           horizons = c(1,7),
                                           lookback = 1:7,
                                           dates = train$arrival_date, 
                                           frequency = "1 day",
                                           dynamic_features = c("day","week"),
                                           groups = "hotel"
                                           )
head(data_test)
```


```{r}
plot(data_train)
```

```{r}
windows <- forecastML::create_windows(data_train, 
                                      window_length = 7*4, 
                                      skip = 7,
                                      include_partial_window = F)

plot(windows, data_train) + theme(legend.position = "none")

```


```{r}
model_function <- function(data, outcome_col = 1) {
  
data <- data[!is.na(data[, 1]), ]

data_train <- data %>% 
              head(-30)
data_test <- data %>% 
             tail(30)

# convert data to matrix
train_matrix <- data.matrix(data_train[,-1, drop = F])
test_matrix <- data.matrix(data_test[,-1, drop = F])

# conver data to Dmatrix
dtrain <- xgb.DMatrix(data = train_matrix, 
                      label = as.matrix(data_train[,1, drop =F]))
dtest <- xgb.DMatrix(data = test_matrix, 
                     label = as.matrix(data_test[,1, drop =F]))

params <- list("objective" = "reg:squarederror")
watchlist <- list(train = dtrain, test = dtest)
  
set.seed(100)
model <- xgboost::xgb.train(data = dtrain, 
                            params = params, 
                            max.depth = 8, 
                            nthread = 2, 
                            nrounds = 30,
                            metrics = "rmse",
                            early_stopping_rounds = 5, 
                            watchlist = watchlist,
                            verbosity = 0)

  return(model)
}
```


```{r}
model_results <- forecastML::train_model(lagged_df = data_train,
                                            windows = windows,
                                            model_name = "xgboost",
                                            model_function = model_function, 
                                            use_future = FALSE)
```

```{r}
summary(model_results$horizon_1$window_1$model)
```

```{r}
prediction_function <- function(model, data_features) {
  x <- xgb.DMatrix(data = data.matrix(data_features))
  data_pred <- data.frame("y_pred" = predict(model, x))  
  return(data_pred)
}
```

```{r}
data_pred_train <- predict(model_results, prediction_function = list(prediction_function), data = data_train) 

```

```{r}
plot(data_pred_train)
```

```{r}
pred_train <- data_pred_train %>% 
  select(date_indices, hotel, demand, demand_pred, model_forecast_horizon) %>% 
  pivot_longer(cols = c(demand, demand_pred), names_to = "result") 
```


```{r}
ggplot(pred_train, aes(x = date_indices, y = value))+
  geom_line( aes(group = result, col = result))+
  facet_grid(hotel~model_forecast_horizon, scales = "free",)+
  tidyquant::theme_tq()
```


```{r}
data_pred_test <- predict(model_results, prediction_function = list(prediction_function), data = data_test) 

```

```{r}
test
```


```{r}
pred_test <- data_pred_test %>% 
  select(forecast_period, hotel, demand_pred, model_forecast_horizon) %>% 
#  pivot_longer(cols = c(demand_pred), names_to = "result") %>%
  rename(arrival_date = forecast_period) %>% 
  left_join(test,by = c("arrival_date", "hotel")) %>% 
    pivot_longer(cols = c(demand_pred, demand), names_to = "result")
```


```{r}
ggplot(pred_test, aes(x = arrival_date, y = value))+
  geom_line( aes(group = result, col = result))+
  facet_grid(hotel~model_forecast_horizon, scales = "free",)+
  tidyquant::theme_tq()
```
